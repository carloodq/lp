{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import community\n",
    "\n",
    "import random\n",
    "from sklearn import svm\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "# train\n",
    "right = pd.read_csv('graph_features_scaled_train.csv')\n",
    "del right['Unnamed: 0']\n",
    "left = pd.read_csv('training_features_6.csv', header = None)\n",
    "train = pd.concat([left, right], axis=1)\n",
    "\n",
    "X_train = train.as_matrix()\n",
    "\n",
    "y_train = [i[2] for i in training_set ]\n",
    "\n",
    "# test\n",
    "right = pd.read_csv('graph_features_scaled_test.csv')\n",
    "del right['Unnamed: 0']\n",
    "left = pd.read_csv('testing_features_6.csv', header = None)\n",
    "test = pd.concat([left, right], axis=1)\n",
    "\n",
    "X_test = test.as_matrix()\n",
    "\n",
    "############################\n",
    "## Feature engineering\n",
    "############################\n",
    "\n",
    "##### Functions\n",
    "def common_neighbors(features, G):\n",
    "    nb_common_neighbors = []\n",
    "    for i in range(features.shape[0]):\n",
    "        a = features['From'][i]\n",
    "        b = features['To'][i]\n",
    "        nb_common_neighbors.append(len(sorted(nx.common_neighbors(G, a, b)))) # ajoute le nombre de voisins communs\n",
    "    return nb_common_neighbors\n",
    "\n",
    "def Jaccard_coef(features, G):\n",
    "    J = []\n",
    "    for i in range(features.shape[0]):\n",
    "        a = features['From'][i]\n",
    "        b = features['To'][i]\n",
    "        pred = nx.jaccard_coefficient(G, [(a, b)])\n",
    "        for u, v ,p in pred:\n",
    "            J.append(p)\n",
    "    return J\n",
    "\n",
    "def betweeness_diff(features, G):\n",
    "\tbtw = nx.betweenness_centrality(G, 50)\n",
    "    btw_diff = []\n",
    "    for i in range(features.shape[0]):\n",
    "        a = features['From'][i]\n",
    "        b = features['To'][i]\n",
    "        btw_diff.append(btw[b] - btw[a])\n",
    "    return btw_diff\n",
    "\n",
    "def in_link_diff(features, G2):\n",
    "    diff = []\n",
    "    for i in range(features.shape[0]):\n",
    "        a = features['From'][i]\n",
    "        b = features['To'][i]\n",
    "        diff.append(len(G2.in_edges(b)) - len(G2.in_edges(a)))\n",
    "    return diff\n",
    "\n",
    "def is_same_cluster(partition, features):\n",
    "    same_cluster = []\n",
    "    for i in range(features.shape[0]):\n",
    "        a = features['From'][i]\n",
    "        b = features['To'][i]\n",
    "        if(partition[a] == partition[b]):\n",
    "            same_cluster.append(1)\n",
    "        else:\n",
    "            same_cluster.append(0)\n",
    "    return same_cluster\n",
    "\n",
    "\n",
    "\n",
    "##### Creation of features\n",
    "\n",
    "## Load graph data\n",
    "with open(\"training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "\n",
    "with open(\"testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]\n",
    "\n",
    "# Create a non directed graph\n",
    "\n",
    "G = nx.Graph()\n",
    "# extract only the linked nodes from the training set\n",
    "linked_nodes = [(i[0], i[1]) for i in training_set if not i[2] in ['0']]\n",
    "G.add_edges_from(linked_nodes)\n",
    "# Extract the nodes that have no link between\n",
    "no_linked_nodes = [i[0] for i in training_set if not i[2] in ['1']]\n",
    "no_linked_nodes.extend([i[1] for i in training_set if not i[2] in ['1']])\n",
    "# add the nodes that have no links\n",
    "## NB: NetworkX ignores any nodes that are already present in G\n",
    "G.add_nodes_from(no_linked_nodes)\n",
    "\n",
    "############################\n",
    "# Training set\n",
    "############################\n",
    "\n",
    "######################\n",
    "# Graph Features\n",
    "\n",
    "# Create the 2 columns \"node from\" & \"node to\"\n",
    "features_train = pd.DataFrame([[i[0], i[1]]for i in training_set])\n",
    "y = [i[2] for i in training_set]\n",
    "features_train.columns = ['From', 'To']\n",
    "\n",
    "# Feature: number of common neighbors\n",
    "number_common_neighbors = common_neighbors(features_train)\n",
    "features_train['Nb_common_neighbors'] = number_common_neighbors\n",
    "\n",
    "# Feature: Jaccard coefficient\n",
    "Jaccard = Jaccard_coef(features_train)\n",
    "features_train['Jaccard_coef'] = Jaccard\n",
    "\n",
    "# Feature: Betweeness centrality\n",
    "# btw_diff = betweeness_diff(features, btw)\n",
    "# features_train['Betweeness_diff'] = btw_diff\n",
    "\n",
    "# Create a directed graph\n",
    "G2 = nx.DiGraph()\n",
    "# Create the graph\n",
    "G2.add_edges_from(linked_nodes)\n",
    "# add the nodes that have no links\n",
    "## NB: NetworkX ignores any nodes that are already present in G\n",
    "G2.add_nodes_from(no_linked_nodes)\n",
    "nx.is_directed(G2)\n",
    "\n",
    "# Feature: In-link difference\n",
    "diff = in_link_diff(features_train, G2)\n",
    "features_train['In_link_diff'] = diff\n",
    "\n",
    "# feature: Is Same Cluster\n",
    "#first compute the best partition\n",
    "partition = community.best_partition(G)\n",
    "same_cluster_train = is_same_cluster(partition, features_train)\n",
    "features_train['Is_same_cluster'] = same_cluster_train\n",
    "\n",
    "## Save graph features_train in a .csv\n",
    "features_train.to_csv('graph_features_train.csv')\n",
    "\n",
    "############################\n",
    "# Word Embeddings\n",
    "\n",
    "# Load text data\n",
    "with open(\"testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]\n",
    "with open(\"training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "\n",
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [element[0] for element in node_info]\n",
    "\n",
    "#training word embeddings on the abstract of the node information\n",
    "abstracts = [element[5] for element in node_info ]\n",
    "print(\"total nulber of abstracts: %d\" %len(abstracts))\n",
    "abstracts_w = [element.lower().split() for element in abstracts]\n",
    "\n",
    "# Word2vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 200    # Word vector dimensionality\n",
    "min_word_count = 40   # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(abstracts_w, workers=num_workers, size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and\n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    #\n",
    "    # Index2word is a list that contains the names of the words in\n",
    "    # the model's vocabulary. Convert it to a set, for speed\n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    #\n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate\n",
    "    # the average feature vector for each one and return a 2D numpy array\n",
    "    #\n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    #\n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    #\n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        if counter%1000. ==0.:\n",
    "            print \"Review %d of %d\" % (counter, len(reviews))\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter + 1.\n",
    "\n",
    "\n",
    "    return reviewFeatureVecs\n",
    "\n",
    "#create word list for each abstract without stop words\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "abstracts_stp =  [[word for word in element.split(\" \") if word.lower() not in stpwds] for element in abstracts ]\n",
    "\n",
    "DataVecs = getAvgFeatureVecs( abstracts_stp, model, num_features )\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "def isselfcite(source_auth, target_auth):\n",
    "    selfcite = 0\n",
    "    for sauth in source_auth:\n",
    "        if sauth in target_auth:\n",
    "            selfcite = 1\n",
    "            break\n",
    "    return selfcite\n",
    "\n",
    "def issamejournal(source_journal, target_journal):\n",
    "\n",
    "    if source_journal == target_journal:\n",
    "        same_journal = 1\n",
    "    else:\n",
    "        same_journal = 0\n",
    "    return same_journal\n",
    "\n",
    "\n",
    "def cosine_similarity(s_1, s_2):\n",
    "    #remove stopwords\n",
    "    s_1 = np.reshape(s_1,(1,-1)  )\n",
    "    s_2 = np.reshape(s_2,(1,-1)  )\n",
    "    return round(cosine(s_1,s_2), 5)\n",
    "\n",
    "# in this baseline we will use 6 basic features:\n",
    "# number of overlapping words in title\n",
    "overlap_title = []\n",
    "\n",
    "# temporal distance between the papers\n",
    "temp_diff = []\n",
    "\n",
    "# number of common authors\n",
    "comm_auth = []\n",
    "\n",
    "#is self citation\n",
    "self_cite = []\n",
    "\n",
    "#is published in same journal\n",
    "same_journal = []\n",
    "\n",
    "#cosine  similarity\n",
    "cosine_sim = []\n",
    "\n",
    "#####\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "counter = 0\n",
    "for i in xrange(len(training_set)):\n",
    "    source = training_set[i][0]\n",
    "    target = training_set[i][1]\n",
    "\n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "\n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "\n",
    "\t# convert to lowercase and tokenize\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "\t# remove stopwords\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "\n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "\n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "\n",
    "    source_journal = source_info[4].lower()\n",
    "    target_journal = target_info[4].lower()\n",
    "\n",
    "    source_abstract = DataVecs[index_source]\n",
    "    target_abstract = DataVecs[index_target]\n",
    "\n",
    "\n",
    "    overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "    self_cite.append(isselfcite(source_auth,target_auth))\n",
    "    same_journal.append(issamejournal(source_journal, target_journal))\n",
    "    cosine_sim.append(cosine_similarity(source_abstract, target_abstract))\n",
    "\n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print counter, \"training examples processsed\"\n",
    "\n",
    "training_features = np.array([overlap_title, temp_diff, comm_auth,cosine_sim,same_journal, self_cite]).T\n",
    "training_features = preprocessing.scale(training_features)\n",
    "\n",
    "np.savetxt('training_features_6.csv', training_features, delimiter=\",\")\n",
    "\n",
    "\n",
    "############################\n",
    "# Testing set\n",
    "############################\n",
    "\n",
    "# Create the 2 columns \"node from\" & \"node to\"\n",
    "features_test = pd.DataFrame([[i[0], i[1]]for i in testing_set])\n",
    "features_test.columns = ['From', 'To']\n",
    "\n",
    "# Feature: number of common neighbors\n",
    "number_common_neighbors_test = common_neighbors(features_test, G)\n",
    "features_test['Nb_common_neighbors'] = number_common_neighbors_test\n",
    "\n",
    "# Feature: Jaccard coefficient\n",
    "Jaccard_test = Jaccard_coef(features_test)\n",
    "features_test['Jaccard_coef'] = Jaccard_test\n",
    "\n",
    "# Feature: Betweenness centrality\n",
    "# btw_diff_test = betweeness_diff(features_test, G)\n",
    "\n",
    "# Feature: In-link difference\n",
    "diff_test = in_link_diff(features_test, G2)\n",
    "features_test['In_link_diff'] = diff_test\n",
    "\n",
    "# Feature: Is same cluster\n",
    "same_cluster_test = is_same_cluster(partition, features_test)\n",
    "features_test['Is_same_cluster'] = same_cluster_test\n",
    "\n",
    "\n",
    "# Save graph_features test into a csv\n",
    "features_test.to_csv('graph_features_test.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################\n",
    "## Text features\n",
    "\n",
    "#transforming test features\n",
    "# number of overlapping words in title\n",
    "overlap_title_test = []\n",
    "\n",
    "# temporal distance between the papers\n",
    "temp_diff_test = []\n",
    "\n",
    "# number of common authors\n",
    "comm_auth_test = []\n",
    "\n",
    "#is self citation\n",
    "self_cite_test = []\n",
    "\n",
    "#is published in same journal\n",
    "same_journal_test = []\n",
    "\n",
    "#cosine  similarity\n",
    "cosine_sim_test = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(testing_set)):\n",
    "    source = testing_set[i][0]\n",
    "    target = testing_set[i][1]\n",
    "\n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "\n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "\n",
    "\t# convert to lowercase and tokenize\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "\t# remove stopwords\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "\n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "\n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "\n",
    "    source_journal = source_info[4].lower()\n",
    "    target_journal = target_info[4].lower()\n",
    "\n",
    "    source_abstract = DataVecs[index_source]\n",
    "    target_abstract = DataVecs[index_target]\n",
    "\n",
    "\n",
    "    overlap_title_test.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff_test.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth_test.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "    self_cite_test.append(isselfcite(source_auth,target_auth))\n",
    "    same_journal_test.append(issamejournal(source_journal, target_journal))\n",
    "    cosine_sim_test.append(cosine_similarity(source_abstract, target_abstract))\n",
    "\n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print counter, \"test examples processsed\"\n",
    "\n",
    "\n",
    "testing_features = np.array([overlap_title_test, temp_diff_test, comm_auth_test,cosine_sim_test,same_journal_test, self_cite_test]).T\n",
    "\n",
    "# scale\n",
    "testing_features = preprocessing.scale(testing_features)\n",
    "\n",
    "np.savetxt('testing_features_6.csv', testing_features, delimiter=\",\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "## Prediction\n",
    "###############################\n",
    "\n",
    "# Load features for train & test sets\n",
    "## Load data\n",
    "with open(\"training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "\n",
    "# train\n",
    "right = pd.read_csv('graph_features_train.csv')\n",
    "del right['Unnamed: 0']\n",
    "left = pd.read_csv('training_features_6.csv', header = None)\n",
    "train = pd.concat([left, right], axis=1)\n",
    "\n",
    "X_train = train.as_matrix()\n",
    "\n",
    "y_train = [i[2] for i in training_set ]\n",
    "\n",
    "# test\n",
    "right = pd.read_csv('graph_features_test.csv')\n",
    "del right['Unnamed: 0']\n",
    "left = pd.read_csv('testing_features_6.csv', header = None)\n",
    "test = pd.concat([left, right], axis=1)\n",
    "\n",
    "X_test = test.as_matrix()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# initialize basic SVM\n",
    "clf = ExtraTreesClassifier(max_features=None, min_samples_leaf= 20, n_estimators = 500, n_jobs= 3)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "# Extra trees classifier\n",
    "clf = ExtraTreesClassifier(max_features=None, min_samples_leaf= 10, n_estimators = 500, n_jobs= 3)\n",
    "cv = cross_validation.cross_val_score(clf, X_train, y_train, cv=5)\n",
    "\n",
    "# XG Boost classifier\n",
    "# 1st tuning\n",
    "gbm = xgb.XGBClassifier(max_depth=6, n_estimators=500, learning_rate=0.01)\n",
    "cv = cross_validation.cross_val_score(gbm, X_train, y_train, cv=5)\n",
    "print np.mean(cv)\n",
    "\n",
    "# 2nd tuning\n",
    "gbm = xgb.XGBClassifier(max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "gbm.fit(X_train, y_train)\n",
    "pred = gbm.predict(X_test)\n",
    "\n",
    "#Grid Search\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "parameters = {'n_estimators':[500,1000],\n",
    "        'learning_rate': [0.05, 0.01, 0.001]}\n",
    "\n",
    "clf = GridSearchCV( xgb.XGBClassifier(max_depth=4), parameters, n_jobs=4, cv=5, verbose = 10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "##########################\n",
    "## Submission\n",
    "##########################\n",
    "predictions = list(pred)\n",
    "predictions = zip(range(len(pred)), predictions)\n",
    "with open(\"improved_predictions2430_3.csv\",\"wb\") as pred1:\n",
    "    csv_out = csv.writer(pred1)\n",
    "    csv_out.writerow([\"ID\", \"category\"])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
